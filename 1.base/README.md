AIGC和大模型行业术语。

### AGI
Artificial General Intelligence，通用人工智能。

### AIGC
AI-Generated Content或AI Generate Content，AI生成/生产内容。

### SOTA
State of  the Art，一个算法领域里的词，达到了某种艺术效果/状态，就是很牛逼，行业领先的意思。

### Ground truth
就是实际值，期望值，是正确答案、参考答案的意思。算法模型的输出（预测）就要跟这个东西去比较（算法模型的目标，就是要无限接近这个值）。

### FLOPS
Floating-point Operations Per Second，每秒浮点数运算次数算力单位。

### MFLOPS
Million FLOPS，每秒百万次浮点运算算力单位。

### GFLOPS
Giga FLOPS，每秒十亿次浮点运算算力单位。

### GPU
Graphics Processing Unit，图形处理器。起初专门设计用于加速图形的渲染，用于计算机游戏和3D领域。与CPU（中央处理器）比，GPU拥有大量的并行处理核心，使得它们在执行大量并行计算时更高效。当前，广泛应用于深度学习领域，用它可以快速加快模型的训练和推理。

### CUDA
Compute Unified Device Architecture，GPU加速计算架构。这是著名公司英伟达提出的一种用于加速图形运算和深度学习运算的软件架构/框架，里面有许多库函数，比如：图形库、数学库、深度学习库、runtime库等。

### Transformer
一个算法模型架构，要点是：基于“注意力机制”的神经网络模型架构。2017年由Google团队在《Attention is All You Need》论文中提出。它的意义：没有它，深度学习今天可能没这么快创造“辉煌”。

### LLM
Large Language Model，大型语言模型。

### VLM
Visual Language Model，视觉语言模型。

### SD
Stable Diffusion（稳定扩散）的缩写，是【文生图】大模型，也是文生图项目。是美国Stability AI 公司的文生图项目，而且开源。22年8月推出至今，迭代了多个版本：SD1.5、SD2.0、SDXL、SD3。

### MJ
Midjourney的缩写，Midjourney，也是很火、很牛的【文生图】大模型的名字。与SD同等作用。现在已经迭代到MJv6.1版本了，许多设计师、产品经理、摄影师们无一不知。

### 多模态
Multimodal，文本、语音、图像、视频这是不同的模态，多模态模型又分为多模态理解和多模态生成。多模态理解：理解文/语音/视频/图像2种及以上，就可以叫做多模态理解；多模态生成：生成的内容文/语音/视频/图像2种及以上，可以称其具备多模态生成能力。

### 大模型幻觉
英文单词是Hallucination（ /həlusɪˈneɪʃən/ ），大模型生成的虚假和不正确信息，在Meta的《RAG》论文中，被提及，是不是首次提出这个词不知道。幻觉分类：上下文回答自相矛盾和不忠于事实的幻觉（瞎说，编造、扭曲事实）。

### prompt
中文叫法叫做：“提示词”，分为用户提示词和系统提示词。用户提示词：就是你与大模型说的话；系统提示词：就是你给大模型定义的身份、角色、工作任务等。

### prompt工程
Prompt Engineering，可以理解为一个项目或一套体系，目标是设计各个领域的效果好的模型提示词，以使得我们更好地运用好模型的各项能力、发挥大模型所长。提示词工程产出的东西，可以是标准的【提示词模版】（Template），也可以是多做一些东西，比如思维链等等。

### token
词元，在机器学习和深度学习领域范畴，代表的是模型所处理的基本单位。在其它范畴如接口开发，代表的是令牌（用户认证的令牌）。

### tokenizer
词元化器，将文本、图像等模态数据，转化为token的过程。

### Embedding
翻译过来是“嵌入”，是将数据（文本、图像、语音、视频等）转换成向量的技术手段。做Embedding的意义：a）转换成向量后可以“语义”计算；b）不同模态间数据能够交流。

### SFT
Supervised Fine-Tuning，有监督模型微调，简称微调，也称对齐，也可以叫“Fine-tuning”。即在预训练大模型基础上，使用一定量的有标记的数据来对预训练大模型进行效果微调，调整的是模型的全部或部分参数。

### zero-shot
零样本学习。意思是说拿着别人训练好的模型，直接“开干”，不用额外提供标注数据，就能完成你的需求。

### few-shot
使用很少的样本，对大模型进行微调。通常是将样本，加在提示词里就行。

### RLHF
Reinforcement Learning from Human Feedback，基于人类反馈的强化学习。强化学习的一种，在目前主流的LLM或多模态大模型如GPT-4o等，均已支持了该机制。强化学习，就是机器学习的一种。

### Instruction prompting
指令微调，是微调的一种方式。指令微调的数据格式（字段）：
* instruction：人类指令（必填）。
* input：人类输入（选填）。
* output：模型回答（必填）。
* system：系统提示词（选填）。
* history：历史交互记录，包含多轮对话中的指令和回答。

### RAG
Retrieval-Augmented Generation，检索增强型生成模型。2020年，由 Meta（原Facebook）这篇论文中提出《Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》。

### Langchain
2022年10月，Langchain团队提出的一套开源的大语言模型训练和应用的框架。其目标是：使大模型应用开发能够更快、更高效。

### Llama Index
一个跟langchain作用一样的大模型开发的解决方案/产品/技术架构。
官网：[https://www.llamaindex.ai/](https://www.llamaindex.ai/)。

### AI Agent 平台
可以支持用户/企业 零代码、低代码方式快速搭建chatbot的平台。如Coze、扣子、昆仑skyreels、百度千帆、阿里百炼、Dify、腾讯元器、智谱清言-智能体、Langchain也算。

### AI Agent 应用
基于大模型技术搭建的智能体应用与用户交互是chatbot形式。

### chatbot
对话机器人、对话助手（你手机里的“豆包”、“文心一言”、“kimi”等），与之前的智能客服、智能问答机器人“外形”长的一样，但是“脑子”变聪明了。

### AI原生(AI Native）
一个新词，意思是从设计之初就集成了人工智能技术，利用AI功能来增强用户体验和提升应用性能的应用程序。比如各类基于LLM的智能语音助手、智能驾驶等应用为了与原来的AI应用做区分。

### chatGPT
OpenAI公司于2022年11月30日推出的一款基于GPT-3.5预训练大模型的通用智能对话助手（chatbot）。具有移动端、网页端、PC客户端几种产品形态。

### 蒸馏
在AI领域，"蒸馏"通常指的是一种模型压缩技术，它通过训练一个较小的模型（称为学生模型）来模仿一个较大、更复杂的模型（称为教师模型）的行为。这种方法可以使得学生模型在保持相对较小的规模的同时，尽可能地复制教师模型的性能，无需存储教师模型的所有参数。为了能够使模型RUN在在资源受限的环境中，如移动设备。

### DDPM
Denoising Diffusion Probabilistic Models，去噪扩散概率模型。文生图技术发展强大的基石、基础之一。

### ComfyUIComfyUI
是一个用户界面框架，它被设计用于与AI生成模型（如Stable Diffusion、FLUX.1等）配合使用，提供给用户一个更加舒适和易于操作的界面。

### CLIP
Contrastive Language-Image Pre-training，一个文本和图像对齐的技术，也是众多「文生图」算法的第一步。是OpenAI公司在2021年一篇叫做《Learning Transferable Visual Models From Natural Language Supervision》中提出的，其是在丰富的、大规模的（4亿个图像-文本对）数据集上训练得到的。

### DiT
Diffusion Transformer，叫做扩散变换器。目前文生图、文生视频技术的主流模型架构。

### LoRA
Low-Rank Adaptation/Adaptor，低秩适应技术。一种大模型的微调手段。措施是冻结一部分参数可用在视觉和NLP领域，在文生图领域里，主要用于控制图像生成的风格，比如3D卡通、中国风、复古风等等。

### ControlNet
控制网络，用于模型输出的引导，多用在文生图领域，用于精确控图。给定参考图(俗称垫图)，让大模型生成图像参照参考图的图像深度、姿势、构图等。

### MoE
Mixture of Experts，专家混合模型架构。有一些知名的大模型，采用了这一架构，比如：MiniMax 的ABAB6，昆仑天工的skyworks。

### CoE
Cross-attention of Experts，专家交叉注意力机制。

### 过拟合
Overfitting，模型在训练数据上表现过好，换到另一份新的数据上表现很差。这是一种模型未训练好的典型体现。造成的原因可能是数据量不足、模型复杂度过高、训练时间过长。

### 欠拟合
Underfitting，模型未能捕捉数据特征，即模型无法充分拟合训练数据。这是另一种模型未训练好的典型体现。造成的模型可能是：模型过于简单、训练数据不足等。

### ASR
Automatic Speech Recognition，语音识别技术。这在大模型出来之前一直就在研究的AI语音领域的技术。

### TTS
Text-to-Speech，文本合成语音技术。还有种说法是“Speech Synthesis”这在大模型出来之前一直就有的东西。

### 容器化技术
容器化技术是一种软件开发实践，它允许开发者将应用程序及其依赖项打包到一个轻量级、可移植的容器中。这些容器在运行时是隔离的，确保了应用程序可以在不同的环境中一致地运行，而不受底层基础设施的影响。相关技术Docker、K8S（Kubernetes）（谷歌设计的）。K8s是一个开源的容器编排平台，用于自动化部署、扩展和管理容器化应用程序。Kubernetes提供了一个运行分布式系统的框架，能够无缝地扩展和管理跨主机集群的容器应用。

### 异构计算
异构，也就是不同的架构。异构计算这个词，经常出现在人工智能领域，尤其是算力这个细分领域。比如政府智慧城市等大项目，那就要面临来自不同厂家的大模型平台/应用（底层是不同品牌的芯片、不同架构的芯片设计），有GPU、有FPGA之类的。那么如何让他们一起计算不浪费计算资源呢？就得需要【异构计算】的方案。

### 弹性计算
"弹性计算"是一种云计算服务，它允许用户根据需求动态地扩展或缩减计算资源。一般大厂提供的云服务，都支持弹性计算（弹性扩缩容），目的是充分合理利用服务器等计算资源。

### 微服务
它就是一种软件开发架构，目的是提高软件开发的效率、灵活性、标准性。具体做法是：将一个应用程序分解为一组小服务，每个服务运行在其独立的进程中，并通过轻量级的通信机制（通常是HTTP RESTful API）进行交互。这些服务围绕特定的业务能力构建，并且可以独立地部署、扩展和更新。

### 分布式集群、分布式计算
在软件领域，为了提高算力利用率（服务器资源利用率），一般都会采用分布式计算这种方案，主要路径是：搭建分布式计算集群。分布式计算集群：每一个节点都是一个计算单元，它们以某种方式组织在一起，形成了个集群。这些节点资源（网络资源、数据资源啥的）可以提供比单个计算机更高的计算能力、存储容量和可靠性。一般有点用户规模量级的应用/平台，其软件部署层面，肯定会采用这种部署方式。




